<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>quiz_1_575_cs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="quiz_1_575_cs_files/libs/clipboard/clipboard.min.js"></script>
<script src="quiz_1_575_cs_files/libs/quarto-html/quarto.js"></script>
<script src="quiz_1_575_cs_files/libs/quarto-html/popper.min.js"></script>
<script src="quiz_1_575_cs_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="quiz_1_575_cs_files/libs/quarto-html/anchor.min.js"></script>
<link href="quiz_1_575_cs_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="quiz_1_575_cs_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="quiz_1_575_cs_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="quiz_1_575_cs_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="quiz_1_575_cs_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="quiz-1-575-cs" class="level1">
<h1>Quiz 1 575 CS</h1>
<section id="language-models" class="level2">
<h2 class="anchored" data-anchor-id="language-models">Language Models</h2>
<ul>
<li>It computes the probability distribution of a sequence of words.
<ul>
<li><span class="math inline">\(P(w_1, w_2, ..., w_t)\)</span></li>
<li>E.g. P(“I have read this book) &gt; P(”Eye have red this book”)</li>
</ul></li>
<li>Can also get the probability of the upcoming word.
<ul>
<li><span class="math inline">\(P(w_t | w_1, w_2, ..., w_{t-1})\)</span></li>
<li>E.g. P(“book” | “I have read this”) &gt; P(“book” | “I have red this”)</li>
</ul></li>
</ul>
<section id="large-language-models" class="level3">
<h3 class="anchored" data-anchor-id="large-language-models">Large Language Models</h3>
<ul>
<li>Large language models are trained on a large corpus of text.</li>
</ul>
</section>
</section>
<section id="markov-model" class="level2">
<h2 class="anchored" data-anchor-id="markov-model">Markov Model</h2>
<ul>
<li><strong>High-level</strong>: The probability of a word depends only on the previous word (forget everything written before that).</li>
<li><strong>Idea</strong>: Predict future depending upon:
<ul>
<li>The current state</li>
<li>The probability of change</li>
</ul></li>
</ul>
<section id="markov-assumption" class="level3">
<h3 class="anchored" data-anchor-id="markov-assumption">Markov Assumption</h3>
<p>Naive probability of a sequence of words: <span class="math display">\[P(w_1, w_2, ..., w_t) = P(w_1)P(w_2|w_1)P(w_3|w_1, w_2)...P(w_t|w_1, w_2, ..., w_{t-1})\]</span></p>
<p>e.g.&nbsp;<span class="math display">\[P(\text{I have read this book}) = P(\text{I})P(\text{have}|\text{I})P(\text{read}|\text{I have})P(\text{this}|\text{I have read})P(\text{book}|\text{I have read this})\]</span></p>
<p>Or simply: <span class="math display">\[P(w_1, w_2, ..., w_t) = \prod_{i=1}^{t} P(w_i|w_{1:i-1})\]</span></p>
<p>But this is hard, so in Markov model (n-grams), we only consider the <code>n</code> previous words. With the assumption:</p>
<p><span class="math display">\[P(w_t|w_1, w_2, ..., w_{t-1}) \approx P(w_t| w_{t-1})\]</span></p>
</section>
<section id="markov-chain-definition" class="level3">
<h3 class="anchored" data-anchor-id="markov-chain-definition">Markov Chain Definition</h3>
<ul>
<li>Have a set of states <span class="math inline">\(S = \{s_1, s_2, ..., s_n\}\)</span>.</li>
<li>A set of discrete initial probabilities <span class="math inline">\(\pi_0 = \{\pi_0(s_1), \pi_0(s_2), ..., \pi_0(s_n)\}\)</span>.</li>
<li>A transition matrix <span class="math inline">\(T\)</span> where each <span class="math inline">\(a_{ij}\)</span> is the probability of transitioning from state <span class="math inline">\(s_i\)</span> to state <span class="math inline">\(s_j\)</span>.</li>
</ul>
<p><span class="math display">\[
T =
\begin{bmatrix}
    a_{11}       &amp; a_{12} &amp; a_{13} &amp; \dots &amp; a_{1n} \\
    a_{21}       &amp; a_{22} &amp; a_{23} &amp; \dots &amp; a_{2n} \\
    \dots \\
    a_{n1}       &amp; a_{n2} &amp; a_{n3} &amp; \dots &amp; a_{nn}
\end{bmatrix}
\]</span></p>
<ul>
<li><strong>Properties</strong>:
<ul>
<li><span class="math inline">\(0 \leq a_{ij} \leq 1\)</span></li>
<li><strong>rows sum to 1</strong>: <span class="math inline">\(\sum_{j=1}^{n} a_{ij} = 1\)</span></li>
<li>columns do not need to sum to 1</li>
<li>This is assuming <strong>Homogeneous Markov chain</strong> (transition matrix does not change over time).</li>
</ul></li>
</ul>
</section>
<section id="markov-chain-tasks" class="level3">
<h3 class="anchored" data-anchor-id="markov-chain-tasks">Markov Chain Tasks</h3>
<ol type="1">
<li>Predict probabilities of sequences of states</li>
<li>Compute probability of being at a state at a given time</li>
<li>Stationary Distribution: Find steady state after a long time</li>
<li>Generation: Generate a sequences that follows the probability of states</li>
</ol>
<section id="stationary-distribution" class="level4">
<h4 class="anchored" data-anchor-id="stationary-distribution">Stationary Distribution</h4>
<ul>
<li>Steady state after a long time.</li>
<li>Basically the eigenvector of the transition matrix corresponding to the eigenvalue 1.</li>
</ul>
<p><span class="math display">\[\pi T = \pi\]</span></p>
<ul>
<li>Where <span class="math inline">\(\pi\)</span> is the stationary probability distribution <br></li>
<li><strong>Sufficient Condition for Uniqueness</strong>:
<ul>
<li>Positive transitions (<span class="math inline">\(a_{ij} &gt; 0\)</span> for all <span class="math inline">\(i, j\)</span>)</li>
</ul></li>
<li><strong>Weaker Condition for Uniqueness</strong>:
<ul>
<li><strong>Irreducible</strong>: Can go from any state to any other state (fully connected)</li>
<li><strong>Aperiodic</strong>: No fixed period (does not fall into a repetitive loop)</li>
</ul></li>
</ul>
</section>
</section>
<section id="learning-markov-models" class="level3">
<h3 class="anchored" data-anchor-id="learning-markov-models">Learning Markov Models</h3>
<ul>
<li>Similar to Naive Bayes, Markov models is just counting</li>
<li>Given <span class="math inline">\(n\)</span> samples/ sequences, we can find:
<ul>
<li>Initial probabilities: <span class="math inline">\(\pi_0(s_i) = \frac{\text{count}(s_i)}{n}\)</span></li>
<li>Transition probabilities: <span class="math inline">\(a_{ij} = \pi(s_i| s_j) = \frac{\text{count}(s_i, s_j)}{\text{count}(s_i)} = \frac{\text{count of state i to j}}{\text{count of state i to any state}}\)</span></li>
</ul></li>
</ul>
</section>
<section id="n-gram-language-model" class="level3">
<h3 class="anchored" data-anchor-id="n-gram-language-model">n-gram language model</h3>
<ul>
<li>Markov model for NLP</li>
<li><code>n</code> in n-gram means <span class="math inline">\(n-1\)</span> previous words are considered
<ul>
<li>e.g.&nbsp;<code>n=2</code> (bigram) means consider current word for the future</li>
<li>DIFFERENT from Markov model definition bigram= markov model with <code>n=1</code> (we normally use this definition in NLP)</li>
</ul></li>
<li>We extend the definition of a “state” to be a sequence of words
<ul>
<li>e.g.&nbsp;“I have read this book” -&gt; bigram states: “I have”, “have read”, “read this”, “this book”</li>
</ul></li>
<li>example: “I have read this book”
<ul>
<li>trigram (n=2): <span class="math inline">\(P(\text{book} | \text{read this})\)</span></li>
<li>n=3: <span class="math inline">\(P(\text{book} | \text{have read this})\)</span></li>
</ul></li>
</ul>
<p><em>Note: n we use above is not the same as n in n-gram</em></p>
<section id="evaluating-language-models" class="level4">
<h4 class="anchored" data-anchor-id="evaluating-language-models">Evaluating Language Models</h4>
<ul>
<li>Best way is to embed it in an application and measure how much the application improves (<strong>extrinsic evaluation</strong>)</li>
<li>Often it is expensive to run NLP pipeline</li>
<li>It is helpful to have a metric to quickly evaluate performance</li>
<li>Most common <strong>intrinsic evaluation</strong> metric is <strong>perplexity</strong>
<ul>
<li><strong>Lower perplexity is better</strong> (means better predictor of the words in test set)</li>
</ul></li>
</ul>
</section>
<section id="perplexity" class="level4">
<h4 class="anchored" data-anchor-id="perplexity">Perplexity</h4>
<p>Let <span class="math inline">\(W = w_1, w_2, ..., w_N\)</span> be a sequences of words.</p>
<p><span class="math display">\[
\text{Perplexity}(W) = P(w_1, w_2, ..., w_N)^{-\frac{1}{N}} \\
= \sqrt[N]{\frac{1}{P(w_1, w_2, ..., w_N)}}
\]</span></p>
<p>For <code>n=1</code> markov model (bigram):</p>
<p><span class="math display">\[P(w_1, w_2, ..., w_N) = \prod_{i=1}^{N} P(w_i|w_{i-1})\]</span></p>
<p>So…</p>
<p><span class="math display">\[
\text{Perplexity}(W) = \sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i|w_{i-1})}}
\]</span></p>
<ul>
<li>Increase <code>n</code> will decrease perplexity =&gt; better model</li>
<li>Too high still bad because of overfitting</li>
</ul>
</section>
</section>
</section>
<section id="applications-of-markov-models" class="level2">
<h2 class="anchored" data-anchor-id="applications-of-markov-models">Applications of Markov Models</h2>
<section id="google-pagerank" class="level3">
<h3 class="anchored" data-anchor-id="google-pagerank">Google PageRank</h3>
<ul>
<li><strong>Idea</strong>: The importance of a page is determined by the importance of the pages that link to it.</li>
<li><strong>Markov Model</strong>: The probability of being on a page at time <span class="math inline">\(t\)</span> depends only on the page at time <span class="math inline">\(t-1\)</span>.</li>
<li><strong>Transition Matrix</strong>: The probability of going from page <span class="math inline">\(i\)</span> to page <span class="math inline">\(j\)</span> is the number of links from page <span class="math inline">\(i\)</span> to page <span class="math inline">\(j\)</span> divided by the number of links from page <span class="math inline">\(i\)</span>.
<ul>
<li>Add <span class="math inline">\(\epsilon\)</span> to all values so that matrix is fully connected</li>
<li>Normalize so sum of each row is 1</li>
</ul></li>
<li><strong>Stationary Distribution</strong>: The stationary distribution of the transition matrix gives the importance of each page.
<ul>
<li>It shows the page’s long-term visit rate</li>
</ul></li>
</ul>
</section>
</section>
<section id="basic-text-preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="basic-text-preprocessing">Basic Text Preprocessing</h2>
<ul>
<li>Text is unstructured and messy
<ul>
<li>Need to “normalize”</li>
</ul></li>
</ul>
<section id="tokenization" class="level3">
<h3 class="anchored" data-anchor-id="tokenization">Tokenization</h3>
<ul>
<li>Sentence segmentation: text -&gt; sentences</li>
<li>Word tokenization: sentence -&gt; words
<ul>
<li>Process of identifying word boundaries</li>
</ul></li>
<li>Characters for tokenization: | Character | Description | | — | — | | Space | Separate words | | dot <code>.</code> | Kind of ambiguous (e.g.&nbsp;<code>U.S.A</code>) | | <code>!</code>, <code>?</code> | Kind of ambiguous too |</li>
<li>How?
<ul>
<li>Regex</li>
<li>Use libraries like <code>nltk</code>, <code>spacy</code>, <code>stanza</code></li>
</ul></li>
</ul>
</section>
<section id="word-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="word-segmentation">Word Segmentation</h3>
<ul>
<li>In NLP we talk about:
<ul>
<li><strong>Type</strong>: Unique words (element in vocabulary)</li>
<li><strong>Token</strong>: Instances of words</li>
</ul></li>
</ul>
</section>
<section id="word-based-vs-character-based-language-models" class="level3">
<h3 class="anchored" data-anchor-id="word-based-vs-character-based-language-models">word-based vs character-based language models</h3>
<table class="table">
<colgroup>
<col style="width: 6%">
<col style="width: 44%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Word-Based</th>
<th>Character-Based</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Advantages</td>
<td>- Faster training and inference due to smaller vocabulary size</td>
<td>- Can handle unseen words (out-of-vocabulary) and typos by generating characters</td>
</tr>
<tr class="even">
<td></td>
<td>- Leverages existing knowledge of grammar and syntax through word relationships</td>
<td>- More flexible for generating creative text formats like code or names</td>
</tr>
<tr class="odd">
<td>Disadvantages</td>
<td>- Requires a large vocabulary, leading to higher memory usage and computational cost</td>
<td>- May struggle with complex morphology (word structure) in some languages</td>
</tr>
<tr class="even">
<td></td>
<td>- Can struggle with unseen words or typos (resulting in “unknown word” tokens)</td>
<td>- May generate grammatically incorrect or nonsensical text due to lack of word-level context</td>
</tr>
</tbody>
</table>
<ul>
<li>n-gram typically have larger state space for word-based models than character-based models</li>
</ul>
</section>
<section id="other-preprocessing-steps" class="level3">
<h3 class="anchored" data-anchor-id="other-preprocessing-steps">Other Preprocessing Steps</h3>
<ul>
<li>Removing stop words</li>
<li>Lemmatization: Convert words to their base form</li>
<li>Stemming: Remove suffixes
<ul>
<li>e.g.&nbsp;automates, automatic, automation -&gt; automat</li>
<li>Not actual words, but can be useful for some tasks</li>
<li>Be careful, because kind of aggressive</li>
</ul></li>
</ul>
</section>
<section id="other-typical-nlp-tasks" class="level3">
<h3 class="anchored" data-anchor-id="other-typical-nlp-tasks">Other Typical NLP Tasks</h3>
<ul>
<li><strong>Part of Speech (POS) Tagging</strong>: Assigning a part of speech to each word</li>
<li><strong>Named Entity Recognition (NER)</strong>: Identifying named entities in text</li>
<li><strong>Coreference Resolution</strong>: Identifying which words refer to the same entity</li>
<li><strong>Dependency Parsing</strong>: Identifying the grammatical structure of a sentence</li>
</ul>
</section>
</section>
<section id="hidden-markov-models" class="level2">
<h2 class="anchored" data-anchor-id="hidden-markov-models">Hidden Markov Models</h2>
<section id="speech-recognition" class="level3">
<h3 class="anchored" data-anchor-id="speech-recognition">Speech Recognition</h3>
<ul>
<li>Python has several libraries for speech recognition.
<ul>
<li>Have a module called <code>SpeechRecognition</code> which can access:
<ul>
<li>Google Web Speech API</li>
<li>Sphinx</li>
<li>Wit.ai</li>
<li>Microsoft Bing Voice Recognition</li>
<li>IBM Speech to Text</li>
</ul></li>
<li>Might need to pay for some of these services</li>
</ul></li>
<li><strong>General Task</strong>: Given a sequence of audio signals, want to recognize the corresponding phenomes/ words
<ul>
<li><strong>Phenomes</strong>: Distinct units of sound
<ul>
<li>E.g. “cat” has 3 phenomes: “k”, “ae”, “t”. “dog” has 3 phenomes: “d”, “aa”, “g”</li>
</ul></li>
<li>English has ~44 phenomes</li>
</ul></li>
<li>It is a <strong>sequence modeling problem</strong></li>
<li>Many modern speech recognition systems use HMM
<ul>
<li>HMM is also still useful in bioinformatics, financial modeling, etc.</li>
</ul></li>
</ul>
</section>
<section id="hmm-definition-and-example" class="level3">
<h3 class="anchored" data-anchor-id="hmm-definition-and-example">HMM Definition and Example</h3>
<ul>
<li><strong>Hidden</strong>: The state is not directly observable
<ul>
<li>e.g.&nbsp;In speech recognition, the phenome is not directly observable. Or POS (Part of Speech) tags in text.</li>
</ul></li>
<li>HMM is specified by a 5-tuple <span class="math inline">\((S, Y, \pi, T, B)\)</span>
<ul>
<li><span class="math inline">\(S\)</span>: Set of states</li>
<li><span class="math inline">\(Y\)</span>: Set of observations</li>
<li><span class="math inline">\(\pi\)</span>: Initial state probabilities</li>
<li><span class="math inline">\(T\)</span>: Transition matrix, where <span class="math inline">\(a_{ij}\)</span> is the probability of transitioning from state <span class="math inline">\(s_i\)</span> to state <span class="math inline">\(s_j\)</span></li>
<li><span class="math inline">\(B\)</span>: Emission probabilities. <span class="math inline">\(b_j(y)\)</span> is the probability of observing <span class="math inline">\(y\)</span> in state <span class="math inline">\(s_j\)</span></li>
</ul></li>
<li>Yielding the state sequence and observation sequence</li>
</ul>
<p><span class="math display">\[\text{State Sequence}:Q = q_1, q_2, ..., q_T \in S\]</span></p>
<p><span class="math display">\[\text{Observation Sequence}: O = o_1, o_2, ..., o_T \in Y\]</span></p>
<section id="hmm-assumptions" class="level4">
<h4 class="anchored" data-anchor-id="hmm-assumptions">HMM Assumptions</h4>
<p><img src="../images/3_hmm.png" width="350"></p>
<ol type="1">
<li>The probability of a particular state depends only on the previous state</li>
</ol>
<p><span class="math display">\[P(q_i|q_0,q_1,\dots,q_{i-1})=P(q_i|q_{i-1})\]</span></p>
<ol start="2" type="1">
<li>Probability of an observation depends <strong>only</strong> on the state.</li>
</ol>
<p><span class="math display">\[P(o_i|q_0,q_1,\dots,q_{i-1},o_0,o_1,\dots,o_{i-1})=P(o_i|q_i)\]</span></p>
<p><strong>Important Notes</strong>:</p>
<ul>
<li>Observations are ONLY dependent on the current state</li>
<li>States are dependent on the previous state (not observations)</li>
<li>Each hidden state has a probability distribution over all observations</li>
</ul>
</section>
<section id="fundamental-questions-for-a-hmm" class="level4">
<h4 class="anchored" data-anchor-id="fundamental-questions-for-a-hmm">Fundamental Questions for a HMM</h4>
<ol type="1">
<li>Likelihood
<ul>
<li>Given <span class="math inline">\(\theta = (\pi, T, B)\)</span> what is the probability of observation sequence <span class="math inline">\(O\)</span>?</li>
</ul></li>
<li>Decoding
<ul>
<li>Given an observation sequence <span class="math inline">\(O\)</span> and model <span class="math inline">\(\theta\)</span>. How do we choose the best state sequence <span class="math inline">\(Q\)</span>?</li>
</ul></li>
<li>Learning
<ul>
<li>Given an observation sequence <span class="math inline">\(O\)</span>, how do we learn the model <span class="math inline">\(\theta = (\pi, T, B)\)</span>?</li>
</ul></li>
</ol>
</section>
<section id="hmm-likelihood" class="level4">
<h4 class="anchored" data-anchor-id="hmm-likelihood">HMM Likelihood</h4>
<ul>
<li>What is the probability of observing sequence <span class="math inline">\(O\)</span>?</li>
</ul>
<p><span class="math display">\[P(O) = \sum\limits_{Q} P(O,Q)\]</span></p>
<p>This means we need all the possible state sequences <span class="math inline">\(Q\)</span></p>
<p><span class="math display">\[P(O,Q) = P(O|Q)\times P(Q) = \prod\limits_{i=1}^T P(o_i|q_i) \times \prod\limits_{i=1}^T P(q_i|q_{i-1})\]</span></p>
<p>This is computationally inefficient. <span class="math inline">\(O(2Tn^T)\)</span></p>
<ul>
<li>Need to find every possible state sequence <span class="math inline">\(n^T\)</span>, then consider each emission given the state sequence <span class="math inline">\(T\)</span></li>
<li><span class="math inline">\(n\)</span> is the number of hidden states</li>
<li><span class="math inline">\(T\)</span> is the length of the sequence</li>
</ul>
<p>To solve this, we use dynamic programming (Forward Procedure)</p>
<section id="dynamic-programming-forward-procedure" class="level5">
<h5 class="anchored" data-anchor-id="dynamic-programming-forward-procedure">Dynamic Programming: Forward Procedure</h5>
<ul>
<li>Find <span class="math inline">\(P(O|\theta)\)</span></li>
<li>Make a table of size <span class="math inline">\(n \times T\)</span> called <strong>Trellis</strong>
<ul>
<li>rows: hidden states</li>
<li>columns: time steps</li>
</ul></li>
<li>Fill the table using the following formula:
<ol type="1">
<li><strong>Initialization</strong>: compute first column (<span class="math inline">\(t=0\)</span>)
<ul>
<li><span class="math inline">\(\alpha_j(0) = \pi_j b_j(o_1)\)</span>
<ul>
<li><span class="math inline">\(\pi_j\)</span>: initial state probability</li>
<li><span class="math inline">\(b_j(o_1)\)</span>: emission probability</li>
</ul></li>
</ul></li>
<li><strong>Induction</strong>: compute the rest of the columns (<span class="math inline">\(1 \leq t &lt; T\)</span>)
<ul>
<li><span class="math inline">\(\alpha_j(t+1) = \sum\limits_{i=1}^n \alpha_i(t) a_{ij} b_j(o_{t+1})\)</span>
<ul>
<li><span class="math inline">\(a_{ij}\)</span>: transition probability from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span></li>
</ul></li>
</ul></li>
<li><strong>Termination</strong>: sum over the last column (<span class="math inline">\(t=T\)</span>)
<ul>
<li><span class="math inline">\(P(O|\theta) = \sum\limits_{i=1}^n \alpha_T(i)\)</span></li>
</ul></li>
</ol></li>
<li>It is computed left to right and top to bottom</li>
<li>Time complexity: <span class="math inline">\(O(2n^2T)\)</span>
<ul>
<li>At each time step, need to compare states to all other states <span class="math inline">\(n^2\)</span></li>
<li>Better compared to the naive approach <span class="math inline">\(O(2Tn^T)\)</span></li>
</ul></li>
</ul>
</section>
</section>
</section>
<section id="supervised-learning-in-hmm" class="level3">
<h3 class="anchored" data-anchor-id="supervised-learning-in-hmm">Supervised Learning in HMM</h3>
<ul>
<li><p>Training data: Set of observations <span class="math inline">\(O\)</span> and set of state sequences <span class="math inline">\(Q\)</span></p></li>
<li><p>Find parameters <span class="math inline">\(\theta = (\pi, T, B)\)</span></p></li>
<li><p>Popular libraries in Python:</p>
<ul>
<li><code>hmmlearn</code></li>
<li><code>pomegranate</code></li>
</ul></li>
</ul>
</section>
<section id="decoding-the-viterbi-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="decoding-the-viterbi-algorithm">Decoding: The Viterbi Algorithm</h3>
<ul>
<li>Given an observation sequence <span class="math inline">\(O\)</span> and model <span class="math inline">\(\theta = (\pi, T, B)\)</span>, how do we choose the best state sequence <span class="math inline">\(Q\)</span>?</li>
<li>Find <span class="math inline">\(Q^* = \arg\max_Q P(O,Q|\theta)\)</span></li>
</ul>
<table class="table">
<colgroup>
<col style="width: 9%">
<col style="width: 47%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Forward Procedure</th>
<th>Viterbi Algorithm</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Purpose</strong></td>
<td>Computes the probability of observing a given sequence of emissions, given the model parameters.</td>
<td>Finds the most likely sequence of hidden states that explains the observed sequence of emissions, given the model parameters.</td>
</tr>
<tr class="even">
<td><strong>Computation</strong></td>
<td>Computes forward probabilities, which are the probabilities of being in a particular state at each time step given the observed sequence.</td>
<td>Computes the most likely sequence of hidden states.</td>
</tr>
<tr class="odd">
<td><strong>Probability Calculation</strong></td>
<td>Sum over all possible paths through the hidden states.</td>
<td>Recursively calculates the probabilities of the most likely path up to each state at each time step.</td>
</tr>
<tr class="even">
<td><strong>Objective</strong></td>
<td>Computes the likelihood of observing a given sequence of emissions.</td>
<td>Finds the most probable sequence of hidden states that explains the observed sequence of emissions.</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Both are dynamic programming algorithms with time complexity <span class="math inline">\(O(n^2T)\)</span></p></li>
<li><p><strong>Viterbi Overview</strong>:</p>
<ul>
<li>Store <span class="math inline">\(\delta\)</span> and <span class="math inline">\(\psi\)</span> at each node in the trellis
<ul>
<li><span class="math inline">\(\delta_i(t)\)</span> is the max probability of the most likely path ending in trellis node at state <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(\psi_i(t)\)</span> is the best possible previous state at time <span class="math inline">\(t-1\)</span> that leads to state <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span></li>
</ul></li>
</ul></li>
</ul>
<p><img src="../images/4_viterbi.png" width="400"></p>
<section id="viterbi-initialization" class="level4">
<h4 class="anchored" data-anchor-id="viterbi-initialization">Viterbi: Initialization</h4>
<ul>
<li><span class="math inline">\(\delta_i(0) = \pi_i b_i(O_0)\)</span>
<ul>
<li>recall <span class="math inline">\(b_i(O_0)\)</span> is the emission probability and <span class="math inline">\(\pi_i\)</span> is the initial state probability</li>
</ul></li>
<li><span class="math inline">\(\psi_i(0) = 0\)</span></li>
</ul>
</section>
<section id="viterbi-induction" class="level4">
<h4 class="anchored" data-anchor-id="viterbi-induction">Viterbi: Induction</h4>
<ul>
<li><p>Best path <span class="math inline">\(\delta_j(t)\)</span> to state <span class="math inline">\(j\)</span> at time <span class="math inline">\(t\)</span> depends on each previous state and their transition to state <span class="math inline">\(j\)</span></p></li>
<li><p><span class="math inline">\(\delta_j(t) = \max\limits_i \{\delta_i(t-1)a_{ij}\} b_j(o_t)\)</span></p>
<ul>
<li><span class="math inline">\(b_j(o_t)\)</span> is the emission probability of observation <span class="math inline">\(o_t\)</span> given state <span class="math inline">\(j\)</span></li>
</ul></li>
<li><p><span class="math inline">\(\psi_j(t) = \arg \max\limits_i \{\delta_i(t-1)a_{ij}\}\)</span></p></li>
</ul>
</section>
<section id="viterbi-conclusion" class="level4">
<h4 class="anchored" data-anchor-id="viterbi-conclusion">Viterbi: Conclusion</h4>
<ul>
<li>Choose the best final state
<ul>
<li><span class="math inline">\(q_t^* = \arg\max\limits_i \delta_i(T)\)</span></li>
</ul></li>
<li>Recursively choose the best previous state
<ul>
<li><span class="math inline">\(q_{t-1}^* = \psi_{q_t^*}(T)\)</span></li>
</ul></li>
</ul>
</section>
</section>
<section id="the-backward-procedure" class="level3">
<h3 class="anchored" data-anchor-id="the-backward-procedure">The Backward Procedure</h3>
<ul>
<li>We do not always have mapping from observations to states (emission probabilities <span class="math inline">\(B\)</span>)</li>
<li>Given an observation sequence <span class="math inline">\(O\)</span> but not the state sequence <span class="math inline">\(Q\)</span>, how do we choose the best parameters <span class="math inline">\(\theta = (\pi, T, B)\)</span>?</li>
<li>Use <strong>forward-backward algorithm</strong></li>
</ul>
<section id="basic-idea" class="level4">
<h4 class="anchored" data-anchor-id="basic-idea">Basic Idea</h4>
<ul>
<li>Reverse of the forward procedure <strong>right to left</strong> but still <strong>top to bottom</strong></li>
<li>Find the probability of observing the rest of the sequence given the current state
<ul>
<li><span class="math inline">\(\beta_j(t) = P(o_{t+1}, o_{t+2}, \dots, o_T)\)</span></li>
</ul></li>
</ul>
</section>
<section id="steps-for-backward-procedure" class="level4">
<h4 class="anchored" data-anchor-id="steps-for-backward-procedure">Steps for Backward Procedure</h4>
<ol type="1">
<li><strong>Initialization</strong>: set all values at last time step to 1
<ul>
<li><span class="math inline">\(\beta_j(T) = 1\)</span></li>
</ul></li>
<li><strong>Induction</strong>: compute the rest of the columns (<span class="math inline">\(1 \leq t &lt; T\)</span>)
<ul>
<li><span class="math inline">\(\beta_i(t) = \sum_{j=1}^N a_{ij}b_{j}(o_{t+1}) \beta_j(t+1)\)</span></li>
</ul></li>
<li><strong>Conclusion</strong>: sum over the first column
<ul>
<li><span class="math inline">\(P(O|\theta) = \sum_{i=1}^N \pi_i b_i(o_1) \beta_i(1)\)</span></li>
</ul></li>
</ol>
</section>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>