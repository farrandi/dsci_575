<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>quiz_2_575</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="quiz_2_575_files/libs/clipboard/clipboard.min.js"></script>
<script src="quiz_2_575_files/libs/quarto-html/quarto.js"></script>
<script src="quiz_2_575_files/libs/quarto-html/popper.min.js"></script>
<script src="quiz_2_575_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="quiz_2_575_files/libs/quarto-html/anchor.min.js"></script>
<link href="quiz_2_575_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="quiz_2_575_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="quiz_2_575_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="quiz_2_575_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="quiz_2_575_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="quiz-2-575-cs" class="level1">
<h1>Quiz 2 575 CS</h1>
<section id="topic-modeling" class="level2">
<h2 class="anchored" data-anchor-id="topic-modeling">Topic Modeling</h2>
<ul>
<li>Identify topics in a collection of documents</li>
<li>Common to use <strong>unsupervised learning</strong> techniques
<ul>
<li>Given hyperparameter <span class="math inline">\(K\)</span>, we want to find <span class="math inline">\(K\)</span> topics.</li>
</ul></li>
<li>In unsupervised, a common model:
<ul>
<li>Input: <span class="math inline">\(D\)</span> documents, <span class="math inline">\(K\)</span> topics</li>
<li>Output:
<ul>
<li>Topic-word association: for each topic, what words describe that topic?</li>
<li>Document-topic association: for each document, what topics are in that document?</li>
</ul></li>
</ul></li>
<li>Common approaches:
<ol type="1">
<li><strong>Latent Semantic Analysis (LSA)</strong></li>
<li><strong>Latent Dirichlet Allocation (LDA)</strong></li>
</ol></li>
</ul>
<section id="latent-semantic-analysis-lsa" class="level3">
<h3 class="anchored" data-anchor-id="latent-semantic-analysis-lsa">Latent Semantic Analysis (LSA)</h3>
<ul>
<li>Singular Value Decomposition (SVD) of the term-document matrix. See <a href="https://mds.farrandi.com/block_5/563_unsup_learn/563_unsup_learn#lsa-latent-semantic-analysis">LSA notes from 563</a>.</li>
</ul>
<p><span class="math display">\[X_{n \times d} \approx Z_{n \times k}W_{k \times d}\]</span></p>
<ul>
<li><span class="math inline">\(n\)</span>: number of documents, <span class="math inline">\(d\)</span>: number of words, <span class="math inline">\(k\)</span>: number of topics</li>
</ul>
</section>
<section id="latent-dirichlet-allocation-lda" class="level3">
<h3 class="anchored" data-anchor-id="latent-dirichlet-allocation-lda">Latent Dirichlet Allocation (LDA)</h3>
<ul>
<li><p>Bayesian, generative, and unsupervised model</p></li>
<li><p><strong>Document-topic distribution</strong> or <strong>topic proportions</strong> <span class="math inline">\(\theta\)</span>:</p>
<ul>
<li>Each document is considered a mixture of topics</li>
</ul></li>
<li><p><strong>Topic-word distribution</strong>:</p>
<ul>
<li>Each topic is considered a mixture of words</li>
<li>This is from all documents</li>
</ul></li>
</ul>
<section id="high-level-lda-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="high-level-lda-algorithm">High-level LDA Algorithm</h4>
<ol start="0" type="1">
<li>Set the number of topics <span class="math inline">\(K\)</span></li>
<li>Randomly assign each word in each document to a topic</li>
<li>For each document <span class="math inline">\(d\)</span>:
<ul>
<li>Choose a distribution over topics <span class="math inline">\(\theta\)</span> from a <strong>Dirichlet prior</strong>
<ul>
<li>Use <strong>dirichlet</strong> distribution because it is conjugate priot (same form as posterior)</li>
</ul></li>
<li>For each word in the document:
<ul>
<li>Choose a topic from the document’s topic distribution <span class="math inline">\(\theta\)</span></li>
<li>Choose a word from the topic’s word distribution <br></li>
</ul></li>
</ul></li>
</ol>
<ul>
<li>Fit using Bayesian inference (most commonly MCMC)</li>
</ul>
</section>
<section id="gibbs-sampling" class="level4">
<h4 class="anchored" data-anchor-id="gibbs-sampling">Gibbs Sampling</h4>
<ul>
<li><p>A Markov Chain Monte Carlo (MCMC) method</p></li>
<li><p>Very accurate, but slow (alternative is <strong>Variational Inference</strong>, which is faster but less accurate)</p></li>
<li><p>Used to approximate the posterior distribution for document-topic and topic-word distributions</p></li>
<li><p><strong>Main steps</strong>:</p>
<ol type="1">
<li><p><strong>Initialization</strong>: Randomly assign each word in each document to a topic</p></li>
<li><p><strong>Update topic-word assignments</strong>:</p>
<ul>
<li>Decrease count of current word in both topic and document distributions</li>
<li>Reassign word to a new topic based on probabilities <img src="../images/5_gibbs_3.png" width="300"></li>
</ul></li>
<li><p><strong>Iterate</strong> until convergence</p></li>
</ol></li>
</ul>
</section>
</section>
<section id="topic-modeling-in-python" class="level3">
<h3 class="anchored" data-anchor-id="topic-modeling-in-python">Topic Modeling in Python</h3>
<ul>
<li>3 Main components:
<ol type="1">
<li>Preprocess corpus</li>
<li>Train LDA (use <code>sklearn</code> or <code>gensim</code>)</li>
<li>Interpret results</li>
</ol></li>
</ul>
<section id="preprocess-corpus" class="level4">
<h4 class="anchored" data-anchor-id="preprocess-corpus">Preprocess Corpus</h4>
<ul>
<li>Crucial to preprocess text data before training LDA</li>
<li>Need tokenization, lowercasing, removing punctuation, stopwords</li>
<li>Optionally, lemmatization or POS tagging</li>
</ul>
</section>
<section id="train-lda" class="level4">
<h4 class="anchored" data-anchor-id="train-lda">Train LDA</h4>
<ul>
<li>With <code>sklearn</code> or <code>gensim</code></li>
<li>Main hyperparameters read more about them in the <a href="https://radimrehurek.com/gensim/models/ldamodel.html">documentation</a>
<ul>
<li><code>num_topics</code>/ <code>K</code>: number of topics</li>
<li><code>alpha</code>: Prior on document-topic distribution
<ul>
<li>High alpha: documents are likely to be a mixture of many topics</li>
<li>Low alpha: documents are likely to be a mixture of few topics</li>
</ul></li>
<li><code>eta</code>: Prior on topic-word distribution
<ul>
<li>High eta: topics are likely to be a mixture of many words</li>
<li>Low eta: topics are likely to be a mixture of few words</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="interpret-results" class="level4">
<h4 class="anchored" data-anchor-id="interpret-results">Interpret Results</h4>
<ul>
<li>Since this is unsupervised, we need to interpret the topics ourselves</li>
<li>Idea is to tell a story to humans and what we should care about and evaluate</li>
<li><strong>Common methods</strong>:
<ul>
<li>Look at the top words in each topic and make judgements
<ul>
<li><strong>Word Intrusion</strong>: Add a random word to the top words and see if it is noticed</li>
</ul></li>
<li>Extrinsic evaluation: Evaluate whether topic nodel with current hyperparameters improves the results of task or not</li>
<li>Quantify topic interpretability with metrics like <strong>Coherence Score</strong>
<ul>
<li>Use <code>Gensim</code>’s <code>CoherenceModel</code> to calculate coherence score</li>
<li>Score is between -1 and 1, higher is better</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
</section>
<section id="recurrent-neural-networks-rnns" class="level2">
<h2 class="anchored" data-anchor-id="recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</h2>
<ul>
<li>Recall when modelling sequences:
<ul>
<li>Order matters</li>
<li>Sequence length can vary</li>
<li>Need to capture long-term dependencies</li>
</ul></li>
<li><strong>Problem with Markov models</strong>:
<ul>
<li>Only capture short-term dependencies</li>
<li>Sparsity problem: if there are a lot of states, the transition matrix will be very sparse</li>
<li>Also need large memory to store the n-grams</li>
<li>MM do not scale well</li>
</ul></li>
<li>To get closer to the ideal language model (closer to ChatGPT), here we will learn <strong>neural sequencing models</strong>.</li>
<li><strong>Problem with Feedforward Neural Networks</strong>:
<ul>
<li>Lose temporal information</li>
<li>All connects are fully connected and flow forward (no loops)</li>
</ul></li>
</ul>
<section id="introduction-to-rnns" class="level3">
<h3 class="anchored" data-anchor-id="introduction-to-rnns">Introduction to RNNs</h3>
<ul>
<li><strong>RNNs</strong> are a type of neural network that can model sequences
<ul>
<li>Similar to NN, it is supervised learning</li>
</ul></li>
<li>Solves the limited memory problem of Markov models
<ul>
<li>Memory only scales with number of words <span class="math inline">\(O(n)\)</span></li>
</ul></li>
<li>They use <strong>recurrent connections</strong> to maintain a state over time. <img src="../images/6_rnn_diag.png" width="350"></li>
<li>Connect the hidden layer to itself</li>
<li>The states above are hidden layers in each time step
<ul>
<li>Similar to HMMs, but state is continuous, high dimensional, and much richer</li>
</ul></li>
<li>Each state contains information about the whole past sequence</li>
<li>Not that different from feedforward NNs
<ul>
<li>Still does forward calculation</li>
<li>Just have new set of weights <span class="math inline">\(U\)</span> that connect previous hidden state to current hidden state</li>
<li>These weights are also trained via backpropagation</li>
</ul></li>
</ul>
</section>
<section id="parameters-in-rnns" class="level3">
<h3 class="anchored" data-anchor-id="parameters-in-rnns">Parameters in RNNs</h3>
<ul>
<li>There are 3 weight matrices in RNNs:
<ul>
<li><strong><span class="math inline">\(W\)</span>: input -&gt; hidden</strong>
<ul>
<li>size: <span class="math inline">\(d_{\text{input}} \times d_{\text{hidden}}\)</span></li>
</ul></li>
<li><strong><span class="math inline">\(V\)</span>: hidden -&gt; output</strong>
<ul>
<li>size: <span class="math inline">\(d_{\text{hidden}} \times d_{\text{output}}\)</span></li>
</ul></li>
<li><strong><span class="math inline">\(U\)</span>: hidden -&gt; hidden</strong>
<ul>
<li>size: <span class="math inline">\(d_{\text{hidden}} \times d_{\text{hidden}}\)</span></li>
</ul></li>
</ul></li>
<li><strong>Important point</strong>: All weights between time steps are shared
<ul>
<li>Allows model to learn patterns that are independent of their position in the sequence</li>
</ul></li>
</ul>
</section>
<section id="forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="forward-pass">Forward Pass</h3>
<ul>
<li>Computing new state <span class="math inline">\(h_t\)</span>:
<ul>
<li><span class="math inline">\(h_t = g(Uh_{t-1} + Wx_t + b_1)\)</span>
<ul>
<li><span class="math inline">\(g()\)</span>: activation function</li>
<li><span class="math inline">\(x_t\)</span>: input at time <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(b_1\)</span>: bias</li>
</ul></li>
</ul></li>
<li>Computing output <span class="math inline">\(y_t\)</span>:
<ul>
<li><span class="math inline">\(y_t = \text{softmax}(Vh_t + b_2)\)</span></li>
</ul></li>
</ul>
<section id="training-rnns" class="level4">
<h4 class="anchored" data-anchor-id="training-rnns">Training RNNs</h4>
<ul>
<li>Since it is supervised, we have: training set, loss function, and backpropagation</li>
<li>Need to tailor backpropagation for RNNs
<ul>
<li>Since hidden layers are connected to themselves, we need to backpropagate through time</li>
<li><strong>Backpropagation Through Time (BPTT)</strong>
<ul>
<li>Unroll the RNN for a fixed number of time steps</li>
<li>Calculate the loss at each time step</li>
<li>Sum the losses and backpropagate</li>
<li>Update the weights</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="rnn-applications" class="level3">
<h3 class="anchored" data-anchor-id="rnn-applications">RNN Applications</h3>
<p>Possible RNN architectures: <img src="../images/6_rnn_arch.png" width="600"></p>
<ul>
<li><strong>Sequence Labelling</strong>:
<ul>
<li>E.g. Named Entity Recognition (NER) or Part-of-Speech (POS) tagging</li>
<li>Many-to-many architecture</li>
<li>Input are pre-trained word embeddings, outputs are tag probabilities by softmax</li>
</ul></li>
<li><strong>Sequence Classification</strong>:
<ul>
<li>E.g. Spam detection or sentiment analysis</li>
<li>Similar to pseudo-code above, feed result of last hidden layer to a feedforward NN</li>
<li>Many-to-one architecture</li>
</ul></li>
<li><strong>Text Generation</strong>:
<ul>
<li>E.g. ChatGPT</li>
<li>One-to-many architecture</li>
<li>Input is a seed, output is a sequence of words</li>
</ul></li>
<li><strong>Image captioning</strong>:
<ul>
<li>E.g. Show and Tell</li>
<li>Many-to-many architecture</li>
<li>Input is an image, output is a sequence of words</li>
</ul></li>
</ul>
</section>
<section id="stacked-rnns" class="level3">
<h3 class="anchored" data-anchor-id="stacked-rnns">Stacked RNNs</h3>
<ul>
<li>Use <em>sequence of outputs</em> from one RNN as the <em>sequence of inputs</em> to another RNN</li>
<li>Generally outperform single-layer RNNs</li>
<li>Can learn different level of abstraction in each layer</li>
<li>Number of layers is a hyperparameter, remember that higher also means more training cost</li>
</ul>
</section>
<section id="bidirectional-rnns" class="level3">
<h3 class="anchored" data-anchor-id="bidirectional-rnns">Bidirectional RNNs</h3>
<ul>
<li>Use case is in POS tagging it is useful to know words both before and after the current word</li>
<li>Bidirectional RNNs have two hidden layers, one for forward and one for backward
<ul>
<li>Combines two independent RNNs</li>
</ul></li>
</ul>
</section>
<section id="problems-with-rnns" class="level3">
<h3 class="anchored" data-anchor-id="problems-with-rnns">Problems with RNNs</h3>
<p>3 Main problems:</p>
<ol type="1">
<li><strong>Hard to remember relevant information</strong>
<ul>
<li>Vanishing gradients because of long sequences</li>
<li>Case example: <code>The students in the exam where the fire alarm is ringing (are) really stressed.</code>
<ul>
<li>Need to retain information that students are plural so use “are”</li>
</ul></li>
</ul></li>
<li><strong>Hard to optimize</strong></li>
<li><strong>Hard to parallelize</strong></li>
</ol>
</section>
</section>
<section id="transformers" class="level2">
<h2 class="anchored" data-anchor-id="transformers">Transformers</h2>
<ul>
<li>Approach to sequence processing without using RNNs or LSTMs</li>
<li><strong>Idea</strong>: Build up richer and richer <strong>contextual representations</strong> of words across series of transformer layers
<ul>
<li><strong>Contextual representation</strong>: Representation of a word that depends on the context in which it appears</li>
</ul></li>
<li><strong>Benefits</strong>:
<ul>
<li><strong>Parallelization</strong>: Can process all words in parallel</li>
<li><strong>Long-range dependencies</strong>: Can learn dependencies between words that are far apart</li>
</ul></li>
<li><strong>Two main components</strong>:
<ul>
<li><strong>Self-attention mechanism</strong></li>
<li><strong>Positional embeddings/encodings</strong></li>
</ul></li>
</ul>
<section id="self-attention-mechanism" class="level3">
<h3 class="anchored" data-anchor-id="self-attention-mechanism">Self-Attention Mechanism</h3>
<ul>
<li><strong>Goal</strong>: To look broadly into the context and tells us how to integrate the representations of context words to build representation of a word</li>
<li><strong>Idea</strong>: Compute attention scores between each pair of words in a sentence
<ul>
<li><strong>Attention score</strong>: How much one word should focus on another word</li>
</ul></li>
</ul>
<section id="high-level-overview" class="level4">
<h4 class="anchored" data-anchor-id="high-level-overview">High-Level Overview</h4>
<p>The basic steps of the self-attention mechanism are as follows:</p>
<ol type="1">
<li>Compare each word to every other word in the sentence (usually by <strong>dot product</strong>)</li>
<li>Apply softmax to derive a probability distribution over all words</li>
<li>Compute a weighted sum of all words, where the weights are the probabilities from step 2</li>
</ol>
<ul>
<li>Operations can be done in parallel</li>
</ul>
</section>
<section id="query-key-value" class="level4">
<h4 class="anchored" data-anchor-id="query-key-value">Query, Key, Value</h4>
<ul>
<li><p><strong>Query</strong> <span class="math inline">\(W^Q\)</span>: Word whose representation we are trying to compute (<strong>current focus of attention</strong>)</p></li>
<li><p><strong>Key</strong> <span class="math inline">\(W^K\)</span>: Word that we are comparing the query to</p></li>
<li><p><strong>Value</strong> <span class="math inline">\(W^V\)</span>: Word that we are trying to compute the representation of (output for the <strong>current focus of attention</strong>)</p></li>
<li><p>We can assume all of them have dimension … [TODO]</p></li>
</ul>
</section>
<section id="self-attention-architecture" class="level4">
<h4 class="anchored" data-anchor-id="self-attention-architecture">Self-Attention Architecture</h4>
<ul>
<li><p>All inputs <span class="math inline">\(a_i\)</span> are connected to each other to make outputs <span class="math inline">\(b_i\)</span></p></li>
<li><p>This is a breakdown of how each input <span class="math inline">\(a_i\)</span> is connected to each output <span class="math inline">\(b_i\)</span> using the query, key, and value <img src="../images/7_sa2.png" width="600"></p>
<ul>
<li>In the example our <strong>query is <span class="math inline">\(a_1\)</span></strong>, and our keys are <span class="math inline">\(a_2\)</span>, <span class="math inline">\(a_3\)</span>, and <span class="math inline">\(a_4\)</span></li>
</ul></li>
</ul>
<p><em>Note</em>: For LLMs, not all the sequences are connected to each other, only words before the current word are connected to the current word.</p>
<section id="breakdown-of-the-steps" class="level5">
<h5 class="anchored" data-anchor-id="breakdown-of-the-steps">Breakdown of the steps</h5>
<ol type="1">
<li><p>Get the <span class="math inline">\(\alpha\)</span> values</p>
<ul>
<li>Can either do a dot product approach (more common)</li>
<li>Or an additive approach with an activation function (like tanh)</li>
</ul></li>
<li><p>Apply softmax to get <span class="math inline">\(\alpha'\)</span> values</p></li>
<li><p>Multiply <span class="math inline">\(\alpha'\)</span> values by the matrix product <span class="math inline">\(W^V \cdot A\)</span> to get the output <span class="math inline">\(b_1\)</span></p></li>
</ol>
</section>
<section id="scaling-the-dot-product" class="level5">
<h5 class="anchored" data-anchor-id="scaling-the-dot-product">Scaling the Dot Product</h5>
<ul>
<li>Result of the dot product can be very large</li>
<li>They are scaled before applying softmax</li>
<li>Common scaling: <span class="math inline">\(score(x_i, x_j) = \frac{x_i \cdot x_j}{\sqrt{d}}\)</span>
<ul>
<li><span class="math inline">\(d\)</span>: Dimensionsionality of the query and key vectors</li>
</ul></li>
</ul>
</section>
</section>
<section id="the-steps-in-matrix-form" class="level4">
<h4 class="anchored" data-anchor-id="the-steps-in-matrix-form">The Steps in Matrix Form</h4>
<p>Let <span class="math inline">\(X\)</span> be matrix of all input <span class="math inline">\(x_i\)</span> vectors (Shape: <span class="math inline">\(N \times d\)</span>)</p>
<ul>
<li><span class="math inline">\(Q_{N \times d_k} = X \cdot W^Q_{d \times d_k}\)</span></li>
<li><span class="math inline">\(K_{N \times d_k} = X \cdot W^K_{d \times d_k}\)</span></li>
<li><span class="math inline">\(V_{N \times d_v} = X \cdot W^V_{d \times d_v}\)</span></li>
</ul>
<p>We can then get <span class="math inline">\(\alpha\)</span> easily by <span class="math inline">\(Q \times K\)</span> (shape: <span class="math inline">\(N \times N\)</span>)</p>
<ul>
<li>Recall <span class="math inline">\(N\)</span> is the number of words in the sentence</li>
</ul>
<p>Then to get the <span class="math inline">\(\text{Self Attention}(Q,K,V) = \text{softmax}(\frac{Q \times K}{\sqrt{d_k}}) \times V\)</span></p>
<p>But for LLMs, we only want to look at the words before the current word, so:</p>
<p><img src="../images/7_qk_llm.png" width="300"></p>
</section>
</section>
<section id="positional-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="positional-embeddings">Positional Embeddings</h3>
<ul>
<li>Using self-attention mechanism, we can learn dependencies between words, but we lose the order of words</li>
<li><strong>Solution</strong>: Add positional embeddings to the input embeddings
<ul>
<li><strong>Positional embeddings</strong>: Embeddings that encode the position of a word in a sentence</li>
</ul></li>
</ul>
</section>
<section id="multi-head-attention" class="level3">
<h3 class="anchored" data-anchor-id="multi-head-attention">Multi-Head Attention</h3>
<ul>
<li><p>Different words in a sentence can relate to each other in different ways simultaneously</p>
<ul>
<li>e.g.&nbsp;“The cat was scared because it didn’t recognize me in my mask”</li>
</ul></li>
<li><p>Single attention layer might not be able to capture all these relationships</p></li>
<li><p>Transformer uses multiple attention layers in parallel</p>
<ul>
<li>Each layer is called a <strong>head</strong></li>
<li>Each head learns different relationships between words</li>
</ul></li>
</ul>
<p><img src="../images/7_multi_att.png" width="500"></p>
</section>
<section id="transformer-blocks" class="level3">
<h3 class="anchored" data-anchor-id="transformer-blocks">Transformer Blocks</h3>
<p><img src="../images/7_trf_blk.png" width="300"></p>
<ul>
<li>Each Transformer block consists of:
<ul>
<li><strong>Multi-head self-attention layer</strong></li>
<li><strong>Feed-forward neural network</strong>:
<ul>
<li><span class="math inline">\(N\)</span> network</li>
<li>1 hidden layer (normally higher dimensionality than input), 2 weight matrices</li>
</ul></li>
<li><strong>Residual connections</strong>
<ul>
<li>Add some “skip” connections because improves learning and gives more information to the next layer</li>
</ul></li>
<li><strong>Layer normalization</strong>
<ul>
<li>Similar to <code>StandardScaler</code>, make mean 0 and variance 1</li>
<li>To keep values in a certain range</li>
</ul></li>
</ul></li>
</ul>
<p><span class="math display">\[
  T^1 = \text{SelfAttention}(X)\\
  T^2 = X + T^1\\
  T^3 = \text{LayerNorm}(T^2)\\
  T^4 = \text{FFN}(T^3)\\
  T^5 = T^4 + T^3\\
  H = \text{LayerNorm}(T^5)
\]</span></p>
<ul>
<li>Input and Output dimensions are matched so they can be “stacked”</li>
</ul>
<section id="transformer-in-llms" class="level4">
<h4 class="anchored" data-anchor-id="transformer-in-llms">Transformer in LLMs</h4>
<p><img src="../images/7_trf_llm.png" width="400"></p>
<ul>
<li>Take output of <span class="math inline">\(h_N\)</span> and get logit vector of shape <span class="math inline">\(1 \times V\)</span> where <span class="math inline">\(V\)</span> is the vocabulary size
<ul>
<li><span class="math inline">\(h_N\)</span> -&gt; unembedding layer -&gt; logit vector -&gt; softmax -&gt; probability distribution</li>
</ul></li>
<li>This probability distribution is used to predict the next word</li>
<li>This is a specific example of a <strong>decoder</strong> in a transformer</li>
</ul>
</section>
</section>
</section>
<section id="types-of-transformers" class="level2">
<h2 class="anchored" data-anchor-id="types-of-transformers">Types of Transformers</h2>
<section id="decoder-only-transformer" class="level3">
<h3 class="anchored" data-anchor-id="decoder-only-transformer">Decoder-only Transformer</h3>
<ul>
<li><strong>Training</strong>: Segment corpus of text into input-output pairs</li>
<li>To predict the next word, given input words</li>
<li>Self-attention only sees words before the current word
<ul>
<li>Use a <strong>causal mask</strong> to prevent the model from looking at future words</li>
</ul></li>
</ul>
<section id="autoregressive-text-generation" class="level4">
<h4 class="anchored" data-anchor-id="autoregressive-text-generation">Autoregressive text generation</h4>
<ul>
<li>Once trained, can generate text autoregressively
<ul>
<li>Incrementally generating words by sampling the next word based on previous choices</li>
<li>Sampling part is similar to generation with Markov models (but with more context and long-range dependencies)</li>
</ul></li>
</ul>
</section>
</section>
<section id="encoder-only-transformer" class="level3">
<h3 class="anchored" data-anchor-id="encoder-only-transformer">Encoder-only Transformer</h3>
<ul>
<li>Mainly designed for a wide range of NLP tasks (e.g., text classification)</li>
<li>It has <strong>bidirectional self-attention</strong>
<ul>
<li>Can learn dependencies between words in both directions</li>
</ul></li>
<li><strong>Training</strong>:
<ul>
<li><strong>“fill in the blank” tasks/ cloze tasks</strong>
<ul>
<li>Model predicts the probability of missing words in a sentence, use cross-entropy loss</li>
</ul></li>
<li>Mask tokens and learn to recover them</li>
<li><strong>Contextual embeddings</strong>: representations created by masked language models
<ul>
<li>Different to single vector embeddings from word2vec</li>
<li>Each word has a different vector depending on the context</li>
</ul></li>
</ul></li>
<li><strong>Transfer learning through fine-tuning</strong>:
<ul>
<li>GPT and BERT models are pre-trained on large corpora (very general)</li>
<li>Can create interfaces from these models to downstream tasks</li>
<li>Either freeze training or make minor adjustments to the model</li>
</ul></li>
</ul>
</section>
<section id="encoder-decoder-transformer" class="level3">
<h3 class="anchored" data-anchor-id="encoder-decoder-transformer">Encoder-Decoder Transformer</h3>
<ul>
<li>For machine translation, summarization, etc.</li>
<li><strong>High level architecture</strong>:
<ul>
<li><strong>Encoder</strong>: Takes input text and creates a representation
<ul>
<li>Similar transformer blocks as in the encoder-only transformer</li>
</ul></li>
<li><strong>Decoder</strong>: Takes the representation and generates the output text
<ul>
<li>More powerful block with extra cross-attention layer that can attend to all encoder words</li>
</ul></li>
<li><strong>Attention mechanism</strong>: Helps the decoder focus on different parts of the input text</li>
</ul></li>
</ul>
</section>
<section id="interim-summary" class="level3">
<h3 class="anchored" data-anchor-id="interim-summary">Interim Summary</h3>
<table class="table">
<colgroup>
<col style="width: 10%">
<col style="width: 24%">
<col style="width: 25%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Decoder-only (e.g., GPT-3)</th>
<th>Encoder-only (e.g., BERT, RoBERTa)</th>
<th>Encoder-decoder (e.g., T5, BART)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Contextual Embedding Direction</td>
<td>Unidirectional</td>
<td>Bidirectional</td>
<td>Bidirectional</td>
</tr>
<tr class="even">
<td>Output Computation Based on</td>
<td>Information earlier in the context</td>
<td>Entire context (bidirectional)</td>
<td>Encoded input context</td>
</tr>
<tr class="odd">
<td>Text Generation</td>
<td>Can naturally generate text completion</td>
<td>Cannot directly generate text</td>
<td>Can generate outputs naturally</td>
</tr>
<tr class="even">
<td>Example</td>
<td>MDS Cohort 8 is the ___</td>
<td>MDS Cohort 8 is the best! → positive</td>
<td>Input: Translate to Mandarin: MDS Cohort 8 is the best! Output: MDS 第八期是最棒的!</td>
</tr>
<tr class="odd">
<td>Usage</td>
<td>Recursive prediction over the sequence</td>
<td>Used for classification tasks, sequence labeling taks and many other tasks</td>
<td>Used for tasks requiring transformations of input (e.g., translation, summarization)</td>
</tr>
<tr class="even">
<td>Textual Context Embeddings</td>
<td>Produces unidirectional contextual embeddings and token distributions</td>
<td>Compute bidirectional contextual embeddings</td>
<td>Compute bidirectional contextual embeddings in the encoder part and unidirectional embeddings in the decoder part</td>
</tr>
<tr class="odd">
<td>Sequence Processing</td>
<td>Given a prompt <span class="math inline">\(X_{1:i}\)</span>, produces embeddings for <span class="math inline">\(X_{i+1}\)</span> to <span class="math inline">\(X_{L}\)</span></td>
<td>Contextual embeddings are used for analysis, not sequential generation</td>
<td>Encode input sequence, then decode to output sequence</td>
</tr>
</tbody>
</table>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>